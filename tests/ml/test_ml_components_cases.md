# Test Cases for ML Components in `src/ml/`

This document outlines test cases for various machine learning components, including data preprocessing, model definition, training, and prediction scripts.

---

## 1. Module: `data_preprocessor.py`

### 1.1. Class: `MLDataPreprocessor`

*   **`__init__(self)`:**
    *   **Test Case 1.1.1: Successful Instantiation**
        *   **Description:** Test basic instantiation of the `MLDataPreprocessor` class.
        *   **Inputs:** `MLDataPreprocessor()`
        *   **Expected Output:** Object is created successfully without errors. No specific attributes are expected to be set by default in the current simple constructor.

*   **`feature_engineer(self, data_df: pd.DataFrame)`:**
    *   **Test Case 1.1.2: Valid Input DataFrame**
        *   **Description:** Test feature engineering with a typical OHLCV DataFrame.
        *   **Inputs:** `data_df` (a `pd.DataFrame` containing 'Open', 'High', 'Low', 'Close', 'Volume' columns).
        *   **Expected Output:**
            *   Returns a `pd.DataFrame`.
            *   New feature columns are created as expected (e.g., `close_lag_1` to `close_lag_5`, `volume_lag_1` to `volume_lag_5`, `pct_change`).
            *   A 'target' column is created (binary, based on next day's close vs current day's close).
            *   Rows with NaNs generated by `shift()` operations (for lags and target) are dropped.
            *   The shape of the output DataFrame is less than the input due to `dropna()`.

    *   **Test Case 1.1.3: DataFrame Missing Required Columns**
        *   **Description:** Test with `data_df` missing one of the essential columns (e.g., 'Close' or 'Volume').
        *   **Inputs:** `data_df` missing 'Close' column.
        *   **Expected Output:** Raises `KeyError` when trying to access the missing column (e.g., `df['Close']`).

    *   **Test Case 1.1.4: Data Too Short for Lags/Target**
        *   **Description:** Test with `data_df` that is too short to generate any non-NaN rows after lagging and target creation (e.g., length < 6, since max lag is 5 and target shifts by -1).
        *   **Inputs:** `data_df` with 5 rows.
        *   **Expected Output:** Returns an empty `pd.DataFrame` because all rows become NaN and are dropped.

*   **`split_data(self, features_df: pd.DataFrame, target_series: pd.Series, test_size: float = 0.2, val_size: float = 0.25)`:**
    *   **Test Case 1.1.5: Standard Split**
        *   **Description:** Test chronological data splitting into train, validation, and test sets.
        *   **Inputs:**
            *   `features_df`: `pd.DataFrame` of features (e.g., 100 rows).
            *   `target_series`: `pd.Series` for the target (100 rows).
            *   `test_size`: 0.2
            *   `val_size`: 0.25 (of the remaining 80%, so 20% of original for val, 60% for train)
        *   **Expected Output:**
            *   Returns `X_train, X_val, X_test, y_train, y_val, y_test`.
            *   Data is split chronologically (no `shuffle=True`).
            *   Shapes are approximately:
                *   `X_train`: (60, num_features), `y_train`: (60,)
                *   `X_val`: (20, num_features), `y_val`: (20,)
                *   `X_test`: (20, num_features), `y_test`: (20,)
            *   Indices of splits maintain chronological order.

    *   **Test Case 1.1.6: Invalid `test_size` or `val_size`**
        *   **Description:** Test with `test_size` or `val_size` outside the (0, 1) range or leading to empty sets.
        *   **Inputs:**
            *   `test_size = 1.1` or `test_size = 0` or `test_size = 1`.
            *   `val_size = 1.0` (would leave train empty).
        *   **Expected Output:** `train_test_split` from `sklearn` will raise `ValueError`.

    *   **Test Case 1.1.7: Small Dataset Split**
        *   **Description:** Test splitting on a very small dataset where splits might result in tiny or empty DataFrames for some sets.
        *   **Inputs:** `features_df` (e.g., 10 rows), `target_series` (10 rows). `test_size=0.2`, `val_size=0.25`.
        *   **Expected Output:**
            *   `X_test` gets 2 rows. `X_train_val` gets 8 rows.
            *   `X_val` gets `ceil(0.25*8) = 2` rows. `X_train` gets 6 rows.
            *   Verify shapes and that no sets (especially train) are unintentionally empty if they shouldn't be.

*   **`scale_data(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame)`:**
    *   **Test Case 1.1.8: Standard Scaling**
        *   **Description:** Test feature scaling using `MinMaxScaler`.
        *   **Inputs:** `X_train_df`, `X_val_df`, `X_test_df` (non-empty DataFrames with numeric features).
        *   **Expected Output:**
            *   Returns `X_train_scaled_df`, `X_val_scaled_df`, `X_test_scaled_df`, and the fitted `scaler` object.
            *   The `scaler` is fitted *only* on `X_train_df`.
            *   Values in scaled DataFrames are generally between 0 and 1.
            *   Original DataFrame structure (columns, index) is preserved in the scaled DataFrames.

    *   **Test Case 1.1.9: Scaling with Constant Feature in Train**
        *   **Description:** Test scaling if a feature in `X_train` is constant.
        *   **Inputs:** `X_train_df` where one column has all identical values.
        *   **Expected Output:** `MinMaxScaler` handles this by scaling that feature to all 0s (if min=max). No errors.

*   **`create_sequences(self, features_df: pd.DataFrame, target_series: pd.Series, sequence_length: int)`:**
    *   **Test Case 1.1.10: Standard Sequence Creation**
        *   **Description:** Test creation of sequences from feature and target data.
        *   **Inputs:**
            *   `features_df`: `pd.DataFrame` of scaled features (e.g., 100 rows, 5 features).
            *   `target_series`: `pd.Series` of targets (100 rows).
            *   `sequence_length`: 10
        *   **Expected Output:**
            *   Returns `X_sequences` (NumPy array) and `y_sequences` (NumPy array).
            *   `X_sequences.shape`: `(100 - 10 + 1, 10, 5)` = `(91, 10, 5)`.
            *   `y_sequences.shape`: `(91,)`.
            *   The target `y_sequences[i]` corresponds to the target at the end of `X_sequences[i]`.

    *   **Test Case 1.1.11: Data Length Less Than `sequence_length`**
        *   **Description:** Test when the input data length is less than `sequence_length`.
        *   **Inputs:**
            *   `features_df`: 5 rows.
            *   `target_series`: 5 rows.
            *   `sequence_length`: 10
        *   **Expected Output:**
            *   `X_sequences` and `y_sequences` are empty NumPy arrays (e.g., shape `(0, 10, num_features)` and `(0,)` respectively).

    *   **Test Case 1.1.12: Data Length Equal To `sequence_length - 1`**
        *   **Description:** Test when input data length is `sequence_length - 1`.
        *   **Inputs:**
            *   `features_df`: 9 rows.
            *   `target_series`: 9 rows.
            *   `sequence_length`: 10
        *   **Expected Output:**
            *   `X_sequences` and `y_sequences` are empty, as no full sequence can be formed. The loop range `(sequence_length - 1, len(feature_data_np))` would be `(9, 9)`, which is empty.

### 1.2. Class: `FinancialDataset(torch.utils.data.Dataset)`

*   **`__init__(self, features: np.ndarray, labels: np.ndarray)`:**
    *   **Test Case 1.2.1: Valid Initialization**
        *   **Description:** Test dataset creation with valid NumPy arrays of sequences and labels.
        *   **Inputs:**
            *   `features`: A NumPy array like `(num_samples, sequence_length, num_features)`.
            *   `labels`: A NumPy array like `(num_samples,)`.
        *   **Expected Output:**
            *   Object is created successfully.
            *   `self.features` is a `torch.Tensor` of `dtype=torch.float32`.
            *   `self.labels` is a `torch.Tensor` of `dtype=torch.float32` and shape `(num_samples, 1)`.

*   **`__len__(self)`:**
    *   **Test Case 1.2.2: Correct Length**
        *   **Description:** Verify `__len__` returns the correct number of samples.
        *   **Inputs:** Initialize `FinancialDataset` with `features` having `N` samples.
        *   **Expected Output:** `len(dataset)` returns `N`.

*   **`__getitem__(self, idx: int)`:**
    *   **Test Case 1.2.3: Correct Item Retrieval**
        *   **Description:** Verify `__getitem__` returns the correct feature sequence and label as tensors at a given index.
        *   **Inputs:**
            *   Initialize `FinancialDataset` with known `features` and `labels`.
            *   Request item at `idx = 0` and `idx = N-1`.
        *   **Expected Output:**
            *   Returns a tuple `(feature_tensor, label_tensor)`.
            *   `feature_tensor` corresponds to `features[idx]` and has shape `(sequence_length, num_features)`.
            *   `label_tensor` corresponds to `labels[idx]` and has shape `(1,)`.
            *   Both tensors are `torch.float32`.

---

## 2. Module: `models/lstm_model.py`

### 2.1. Class: `LSTMModel(nn.Module)`

*   **`__init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int, dropout_prob: float)`:**
    *   **Test Case 2.1.1: Successful Instantiation**
        *   **Description:** Test model instantiation with a set of valid hyperparameters.
        *   **Inputs:** `input_size=10, hidden_size=20, num_layers=2, output_size=1, dropout_prob=0.2`.
        *   **Expected Output:** Model object is created successfully. Layers (`lstm`, `dropout`, `fc`, `sigmoid` if `output_size=1`) are initialized.

*   **`forward(self, x: torch.Tensor)`:**
    *   **Test Case 2.1.2: Forward Pass Execution**
        *   **Description:** Test the forward pass with a sample input tensor.
        *   **Inputs:**
            *   Instantiate `LSTMModel` (e.g., `input_size=10, hidden_size=20, num_layers=1, output_size=1, dropout_prob=0.0`).
            *   `x`: A `torch.Tensor` of shape `(batch_size=4, sequence_length=15, input_size=10)`.
        *   **Expected Output:**
            *   Returns an output `torch.Tensor`.
            *   Output tensor shape is `(batch_size=4, output_size=1)`.
            *   No runtime errors during the forward pass.
            *   If `output_size=1` and sigmoid is active, output values are between 0 and 1.

    *   **Test Case 2.1.3: Forward Pass with Multi-Layer LSTM and Dropout**
        *   **Description:** Test forward pass with multiple LSTM layers and dropout enabled.
        *   **Inputs:**
            *   Instantiate `LSTMModel` (e.g., `input_size=10, hidden_size=20, num_layers=2, output_size=1, dropout_prob=0.2`).
            *   `x`: A `torch.Tensor` of shape `(batch_size=4, sequence_length=15, input_size=10)`.
        *   **Expected Output:**
            *   Returns an output `torch.Tensor` of shape `(batch_size=4, output_size=1)`.
            *   No runtime errors. (Ensures dropout layers are correctly configured).

    *   **Test Case 2.1.4: Model Parameters Update (Conceptual)**
        *   **Description:** Conceptually, model parameters (weights and biases) should be learnable and update after a backward pass and optimizer step.
        *   **Setup (Conceptual):** Requires a minimal training loop: define a loss function, optimizer, perform a forward pass, calculate loss, call `loss.backward()`, call `optimizer.step()`.
        *   **Expected Behavior:** After the optimizer step, the values of `model.parameters()` should have changed from their initial values. This is more of an integration test snippet than a unit test of `forward`.

---

## 3. Module: `train.py` (Script-level)

### 3.1. `train_model(args)` / `if __name__ == '__main__':`

*   **Test Case 3.1.1: Successful Run with Dummy Data**
    *   **Description:** Test the training script with dummy data for a minimal number of epochs.
    *   **Inputs (Command Line Args / `args` object):**
        *   `--data_path dummy`
        *   `--num_dummy_rows 100` (ensure enough for sequence_length + splits)
        *   `--epochs 2`
        *   `--batch_size 16`
        *   `--sequence_length 10`
        *   `--hidden_size 8`
        *   `--num_layers 1`
        *   `--model_save_path "test_output/dummy_model.pth"`
        *   `--early_stopping_patience 3`
    *   **Expected Output:**
        *   Script runs to completion without unhandled errors.
        *   Log messages indicating training progress (epoch loss, validation loss).
        *   A model file (e.g., `test_output/dummy_model.pth`) is created.
        *   A scaler file (e.g., `test_output/dummy_model_scaler.joblib`) is created.
        *   An input size file (e.g., `test_output/dummy_model_input_size.json`) is created.
        *   (Cleanup: Ensure `test_output` directory and its contents are removed after test).

*   **Test Case 3.1.2: Non-Existent Data Path**
    *   **Description:** Test script behavior when `--data_path` points to a non-existent file.
    *   **Inputs (Command Line Args):** `--data_path "path/to/nonexistent/data.csv"`
    *   **Expected Output:**
        *   Script exits gracefully.
        *   An error message is printed indicating the data file was not found (e.g., from the `try-except FileNotFoundError` block in `train_model`).

*   **Test Case 3.1.3: Invalid Hyperparameter Values (via Argparse)**
    *   **Description:** Test if `argparse` handles invalid hyperparameter formats or choices (if applicable).
    *   **Inputs (Command Line Args):**
        *   `--epochs -1` (if not caught by argparse type, then by script logic)
        *   `--learning_rate "not_a_float"`
    *   **Expected Output:**
        *   `argparse` should raise an error for type mismatches.
        *   If a value passes argparse but is logically invalid (e.g., negative epochs not caught by `type=int`), the script might error later or behave unexpectedly if not validated internally. (Current script may not have explicit validation for all numeric arg ranges beyond type).

*   **Test Case 3.1.4: Loss Decrease (Conceptual on Dummy Data)**
    *   **Description:** Verify that on a sufficiently simple dummy dataset, the training and validation loss show a tendency to decrease over a few epochs.
    *   **Inputs:** Similar to 3.1.1, but perhaps `epochs=5` or `10`.
    *   **Expected Behavior:**
        *   Observe logged training and validation losses. While not guaranteed to be perfectly smooth, a general downward trend should be visible if the model is learning. This is a basic sanity check, not a rigorous test of convergence.

*   **Test Case 3.1.5: Early Stopping Trigger**
    *   **Description:** Test if early stopping is triggered when validation loss does not improve.
    *   **Inputs:**
        *   `--epochs 20`
        *   `--early_stopping_patience 3`
        *   (May require a dataset or learning rate that causes validation loss to plateau or worsen quickly).
    *   **Expected Output:**
        *   Script stops training before completing all 20 epochs.
        *   A log message indicates early stopping was triggered.
        *   The saved model corresponds to the epoch with the best validation loss before stopping.

---

## 4. Module: `predict.py`

### 4.1. Function: `get_predictions(...)`

*   **Test Case 4.1.1: Valid Prediction Scenario**
    *   **Description:** Test prediction generation with valid raw data and a path to a trained model and its artifacts.
    *   **Setup:**
        1.  A trained model (`test_model.pth`), its scaler (`test_model_scaler.joblib`), and input size file (`test_model_input_size.json`) must exist (e.g., from a successful `train.py` run like Test Case 3.1.1).
    *   **Inputs:**
        *   `raw_data_df`: A `pd.DataFrame` with recent OHLCV data (at least `sequence_length + some_buffer_for_lags` rows).
        *   `model_path`: "path/to/test_model.pth"
        *   `sequence_length`, `hidden_size`, `num_layers`, `dropout`: Values matching the `test_model.pth`.
    *   **Expected Output:**
        *   Returns a `pd.Series` of predictions (0s or 1s).
        *   The Series index should be a `DatetimeIndex` corresponding to the dates for which predictions are made.
        *   No runtime errors.

*   **Test Case 4.1.2: Non-Existent Model Path**
    *   **Description:** Test behavior when the provided `model_path` (or its derived artifact paths) does not exist.
    *   **Inputs:** `model_path`: "path/to/nonexistent_model.pth"
    *   **Expected Output:** Raises `FileNotFoundError` (from `get_predictions`'s checks).

*   **Test Case 4.1.3: `raw_data_df` Too Short for Sequences**
    *   **Description:** Test with input data that is too short to form even one full sequence after feature engineering.
    *   **Inputs:**
        *   `raw_data_df`: DataFrame with fewer rows than `sequence_length` + lags from feature engineering.
        *   Valid `model_path` and other parameters.
    *   **Expected Output:**
        *   Returns an empty `pd.Series` (as per current implementation: "No sequences created...").

*   **Test Case 4.1.4: `raw_data_df` Leading to All NaN Features**
    *   **Description:** Test with input data that, after feature engineering or scaling, results in feature sets that are all NaN (if such a scenario is plausible).
    *   **Inputs:** `raw_data_df` designed to produce all NaNs post-processing.
    *   **Expected Output:**
        *   Handles gracefully. Likely results in empty sequences or an empty prediction Series. (Current `MLDataPreprocessor.feature_engineer` drops NaNs, so this might be hard to achieve unless the initial data is extremely sparse).

### 4.2. Script-level (`main_cli()` via `if __name__ == '__main__':`)

*   **Test Case 4.2.1: Successful CLI Prediction**
    *   **Description:** Test the prediction script using command-line arguments.
    *   **Setup:** Requires a trained model and artifacts (as in 4.1.1).
    *   **Inputs (Command Line Args):**
        *   `--model_path "path/to/test_model.pth"`
        *   `--data_path "path/to/test_data.csv"` (or `--data_path dummy`)
        *   `--sequence_length`, `--hidden_size`, etc., matching the model.
    *   **Expected Output:**
        *   Script runs to completion.
        *   Prints predictions to standard output (or saves to a file if that functionality were added).
        *   No unhandled errors.

*   **Test Case 4.2.2: Missing Required CLI Arguments**
    *   **Description:** Test script execution with missing required command-line arguments (e.g., `--model_path`).
    *   **Inputs (Command Line Args):** Omit `--model_path`.
    *   **Expected Output:** `argparse` should raise an error indicating the missing argument. Script exits with a non-zero status.

---
